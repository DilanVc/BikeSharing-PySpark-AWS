{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97536d1f-2160-4206-86b4-8e7f2f833554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dilanveracruz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419364be-cea7-49e1-b81f-d8bde1329430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kafka Consumer (Separate Code)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType, IntegerType, DoubleType, LongType\n",
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba24d5e4-cab4-43ab-90ff-75dfc70ff40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a Spark session using Python3 (ipykernel) kernel\n",
    "#To configurate the connection between Apache Kafka and Pyspark, it is necessary run 4 jar files\n",
    "#These files can be downloaded from browser\n",
    "\n",
    "spark = SparkSession.builder.appName('streaming')\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.0')\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-token-provider-kafka-0-10_2.13:3.4.0')\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.13:3.4.0')\\\n",
    "    .config('spark.jars.packages', 'org.apache.kafka:kafka-clients:3.4.0')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d75a3-92eb-4255-9cd3-2a33a85e32fd",
   "metadata": {},
   "source": [
    "##### Set up the KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a04c4397-c1fb-44cf-89a4-77c7a6c395af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Kafka consumer\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('my-topic',\n",
    "                         bootstrap_servers = ['localhost:9092'],\n",
    "                         group_id='2.13',\n",
    "                         api_version=(0,10),\n",
    "                         #max_poll_records=60000,\n",
    "                         consumer_timeout_ms=10000,\n",
    "                         session_timeout_ms = 600000,\n",
    "                         request_timeout_ms = 800000,\n",
    "                         connections_max_idle_ms = 900000,\n",
    "                         #enable_auto_commit=True, \n",
    "                         #value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b0891c8-5f26-4df1-82dc-d6772a165be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    pandas_df = batch_df.toPandas()\n",
    "    processed_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d300781e-d54b-447e-9590-af448c74a709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 03:25:57 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the schema for the data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, LongType\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", IntegerType(), nullable=True),\n",
    "    StructField(\"station_name\", IntegerType(), nullable=True),\n",
    "    StructField(\"area_cleaned\", IntegerType(), nullable=True),\n",
    "    StructField(\"Hour\", IntegerType(), nullable=True),\n",
    "    StructField(\"Day\", IntegerType(), nullable=True),\n",
    "    StructField(\"DayofWeek\", IntegerType(), nullable=True),\n",
    "    StructField(\"Month\", IntegerType(), nullable=True),\n",
    "    StructField(\"mean\", DoubleType(), nullable=True),\n",
    "    StructField(\"press\", DoubleType(), nullable=True),\n",
    "    StructField(\"num_rentals\", LongType(), nullable=True),\n",
    "    StructField(\"station_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"sunshine\", DoubleType(), nullable=True),\n",
    "    StructField(\"cloud\", DoubleType(), nullable=True),\n",
    "    StructField(\"prec\", DoubleType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"end_station_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "#Get the data sent by the producer\n",
    "messages_processed = 0\n",
    "max_messages = 1 #108652\n",
    "while messages_processed < max_messages: \n",
    "    messages = consumer.poll(timeout_ms=1800)\n",
    "    if messages:\n",
    "        batch_df = spark.createDataFrame([], schema)\n",
    "        for message in messages.values():\n",
    "            for msg in message:\n",
    "                value = msg.value.decode('utf-8')\n",
    "                json_rdd = spark.sparkContext.parallelize([value])\n",
    "                df = spark.read.json(json_rdd, schema=schema)\n",
    "                batch_df = batch_df.union(df)\n",
    "                messages_processed += 1\n",
    "\n",
    "        process_batch(batch_df, batch_id=None)\n",
    "        consumer.commit()\n",
    "\n",
    "# Close the consumer\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1588611-c494-4396-9bc0-db9ac66b9324",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 03:32:58 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "23/05/28 03:32:58 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "23/05/28 03:32:58 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "23/05/28 03:33:00 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "23/05/28 03:33:06 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "[Stage 48:====================================================>   (88 + 6) / 94]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+----+---+---------+-----+----+-----+-----------+----------+--------+-----+----+--------+---------+--------------+\n",
      "|    date|station_name|area_cleaned|Hour|Day|DayofWeek|Month|mean|press|num_rentals|station_id|sunshine|cloud|prec|latitude|longitude|end_station_id|\n",
      "+--------+------------+------------+----+---+---------+-----+----+-----+-----------+----------+--------+-----+----+--------+---------+--------------+\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          2|       115|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          3|       259|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          1|       269|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          2|       677|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          2|       277|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          2|       449|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  10| 25|        7|    1|null| null|          1|       689|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          2|       381|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          1|       711|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          2|       808|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          1|       686|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          1|        88|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          3|       639|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          3|       821|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          4|       581|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  11| 25|        7|    1|null| null|          2|       620|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  12| 25|        7|    1|null| null|          3|        43|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  12| 25|        7|    1|null| null|          3|       139|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  12| 25|        7|    1|null| null|          4|       216|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "|20200125|        null|        null|  12| 25|        7|    1|null| null|          4|       694|     0.0|  8.0| 0.2|    null|     null|          null|\n",
      "+--------+------------+------------+----+---+---------+-----+----+-----+-----------+----------+--------+-----+----+--------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "batch_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0025c8a5-b6fb-47bb-8bef-76d52c020c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- station_name: integer (nullable = true)\n",
      " |-- area_cleaned: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- DayofWeek: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- press: double (nullable = true)\n",
      " |-- num_rentals: long (nullable = true)\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- sunshine: double (nullable = true)\n",
      " |-- cloud: double (nullable = true)\n",
      " |-- prec: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac5af9-e405-43f6-8863-5abb0af057d6",
   "metadata": {},
   "source": [
    "##### Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8338f986-852e-4d80-8880-96d210e1709a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/spark/jars/spark-core_2.12-3.4.0.jar) to field java.math.BigInteger.mag\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "GBT_model = PipelineModel.load('model-GBT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce70f6b-adae-490c-8671-fb27a81d1e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 03:08:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/05/28 03:08:47 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:08:47 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:08:49 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:08:53 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:09:09 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:10:20 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "23/05/28 03:16:23 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "[Stage 32:=====================================================>(879 + 4) / 883]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 rows)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pred_stream = GBT_model.transform(batch_df)\n",
    "pred_stream.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a045e-5a4f-45c5-84ce-9dd1c4e19d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_plot = pred_stream.groupBy('area_cleaned') \\\n",
    "                .agg(\n",
    "                     sum('prediction').alias('Estimated Value')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd51ee6-3349-499e-a4f0-c1a8c359005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot = grouped_plot.orderBy(F.col('Estimated Value').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6f5f4-6f5a-4774-b04f-522ca71f7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256728f6-456c-4278-a929-7b1a78cdd8cc",
   "metadata": {},
   "source": [
    "##### Rentals prediction in each area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ede0a3-fdc8-400f-b0e4-0cd399cc097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Sample data\n",
    "\n",
    "\n",
    "df_plot = grouped_plot.limit(10).toPandas()\n",
    "\n",
    "# Set the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(len(df_plot['area_cleaned']))\n",
    "\n",
    "\n",
    "# Plot the real values\n",
    "rects1 = ax.barh(index, df_plot['Estimated Value'], bar_width, label='Previsão')\n",
    "\n",
    "\n",
    "# Set y-axis labels\n",
    "ax.set_yticks(index + bar_width / 2)\n",
    "ax.set_yticklabels(df_plot['area_cleaned'])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Valores')\n",
    "ax.set_ylabel('Áreas')\n",
    "ax.set_title('Número de alugueres para cada área')\n",
    "\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123d176-1528-4fcf-a589-649424a75636",
   "metadata": {},
   "source": [
    "##### Rental prediction for the next business day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102e634-83c7-4740-97bf-20fd98d7c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter = pred_stream.filter(F.col('date') == 20200102)\n",
    "first_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9b7eb-c099-432a-b7b1-ed0931753960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = ( first_filter\n",
    "                .groupby('Hour')\n",
    "                .count()\n",
    "                .sort('Hour', ascending=True)\n",
    "                .toPandas()\n",
    "          )\n",
    "plotBar(df_plot, 'Hour', 'count')\n",
    "plt.title('Previsão de alugueres por hora durante o dia 2 de janeiro de 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb43ad8-b87a-4bc0-906a-9c2feb0b2247",
   "metadata": {},
   "source": [
    "##### Area with most demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd294767-31da-47ef-a68d-563101456663",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot = first_filter.groupBy('area_cleaned') \\\n",
    "                .agg(\n",
    "                     sum('prediction').alias('Estimated Value')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d8dee-5e6a-4923-8522-32b156284f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot = grouped_plot.orderBy(F.col('Estimated Value').desc())\n",
    "grouped_plot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b315bb-3fec-4f6b-923e-16a770cc967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = grouped_plot.limit(10).toPandas()\n",
    "plotBar(df_plot, 'area_cleaned', 'Estimated Value')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Valores Estimados')\n",
    "plt.xlabel('Áreas')\n",
    "plt.title('Áreas previstas com mais alugueres durante o dia 2 de janeiro de 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c91ac3-ad7d-4140-ad3b-967210a1c959",
   "metadata": {},
   "source": [
    "##### Overview of the rentals during the first two weeks of January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0796040c-5096-4d44-9e75-5f35ecd74add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import sum,avg,max,count \n",
    "\n",
    "grouped_plot = pred_stream.groupBy('date') \\\n",
    "                .agg(sum('prediction').alias('Estimated Value')\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7610770-5c7c-4fd5-9bfa-ae0685f9c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot = grouped_plot.sort('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfd33d-0a45-41f0-a53e-6b917aa2008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot = grouped_plot.withColumn(\n",
    "    \"date\", \n",
    "    F.to_date(F.col(\"date\").cast(\"string\"), \"yyyyMMdd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7080ecc-2788-4906-94ae-bad2f7ad47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_plot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ce410-a647-4b1c-90e6-f9e5e77dc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "df_plot = grouped_plot.toPandas()\n",
    "# Create the plot\n",
    "plt.plot(df_plot['date'], df_plot['Estimated Value'], label='Valores previstos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c3106-d4ec-4f9c-a7ed-15399f8c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set labels and title\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valores')\n",
    "plt.title('Número estimado de alugueres por data')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39850f16-7f97-476d-a641-624b3c718f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
